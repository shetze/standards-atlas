#!/usr/bin/env python3

import sys
import argparse
import csv
import re
import logging
import hashlib
#import ollama
import json
import numpy as np
from natsort import natsorted
from IntelliDoc.Clause import Clause, ClauseID, ClauseHeading
from IntelliDoc.KnowledgeDomain import KnowledgeDomain, DocTree
from IntelliDoc.HeadingFactory import HeadingFactory
from IntelliDoc.Relationship import Relationship as rel
from IntelliDoc.RamalamaClient import RamaLama

parser = argparse.ArgumentParser(
    prog="tokenize_standard",
    description="Tool to cut MD formatted standard text into subclauses.\nThe tool helps to sanitize and complete the MD formatted text based on the standards-atlas structural data.",
)
parser.add_argument(
    "-H",
    "--harvest",
    help="Harvest mode: Headings found in the MD document overwrite the specific ones that may have been specified in the standard atlas.",
)
parser.add_argument(
    "-g", "--generate", help="Generate mode: Missing headings will be generated."
)
parser.add_argument(
    "-l",
    "--llm-model",
    nargs="?",
    default="nemotron",
    help="The LLM model to be used. llama3.1 ist faster, nemotron is more accurate.",
)
parser.add_argument(
    "-c",
    "--content",
    nargs="?",
    default="csv/heading-data.csv",
    help="Filename for CSV formatted input data defining the content structure of the documents to be tokenized.",
)
parser.add_argument(
    "-w",
    "--weights",
    nargs="?",
    default="csv/weights.csv",
    help="Filename for CSV formatted input data defining the weights of each sentence in th documents to be tokenized.",
)
parser.add_argument(
    "-r",
    "--relations",
    nargs="?",
    default="csv/relations.csv",
    help="Filename for CSV formatted input data defining the relations between clauses.",
)
parser.add_argument(
    "-d",
    "--refmap",
    nargs="?",
    default="csv/uid-ref-map.csv",
    help="Filename for CSV formatted input data providing the doorstop IDs for the clauses.",
)
parser.add_argument(
    "-i", "--interactive", help="Interactive generation mode, works good with llama3.1"
)
parser.add_argument(
    "-b", "--bulk", help="Bulk mode for heading generation, works good with nemotron"
)

args = parser.parse_args()

logger = logging.getLogger("IntelliDoc")
logging.basicConfig(filename="Tokenizer.log", level=logging.INFO)
content = {}
status = {}
clauseIndex = {}
knowledgeDomains = {}
standardDocs = {}
part = ""
partNr = "x"
llm = RamaLama("llama3.2:1b")

typedict = {
    "u": "text",
    "r": "requirement",
    "s": "scope definition",
    "o": "objective",
    "t": "term definition",
    "c": "clause",
}

iso_pattern = r"([A-Z\s]+\s+\d\d\d\d\d?)-?(\d*):\d\d\d\d\s+([1-9A-Z][0-9.]*)"
iso_regex = re.compile(iso_pattern)

domainDocuments = {
    "railway": {
        "standard": {
            "EN 50126": None,
            # 'EN 50128' : None,
            "EN 50129": None,
            # 'EN 50657' : None,
            "EN 50716": None,
        },
        "generic": {},
    },
    "industry": {"standard": {"IEC 61508": None}, "generic": {}},
    "automotive": {
        "standard": {"ISO 26262": None, "ISO PAS 8926": None},
        "generic": {},
    },
    "generic": {"standard": {}, "generic": {}},
}
domainDocuments1 = {
    "infotec": {"standard": {"IEC 11889": None}, "generic": {}},
}


def load_content_structure(contentfile=args.content):
    with open(contentfile, newline="") as csvfile:
        tocreader = csv.reader(csvfile, delimiter=";", quotechar="|")
        for row in tocreader:
            entry = {}
            standard = ""
            chapter = ""
            logger.debug(row)
            clauseID = row[2]
            title = row[3]
            typeID = row[4]
            match = iso_regex.match(clauseID)
            if match:
                standard = match[1]
                if match[2] == "":
                    part = ""
                    partNr = "x"
                else:
                    part = " part " + match[2]
                    partNr = match[2]
                chapter = match[3]
            else:
                logger.warning(f"parse error for row {row}")
                continue
            clause = Clause(clauseID, title, clauseType=typeID)
            clauseIndex[clauseID] = clause
            docSeries = standard.replace(" ", "")
            for domain in domainDocuments.keys():
                if standard in domainDocuments[domain]["standard"].keys():
                    if domain in knowledgeDomains.keys():
                        knowledgeDomains[domain].addClause(clause, standard)
                    else:
                        knowledgeDomains[domain] = KnowledgeDomain(
                            domain, domainDocuments[domain], clauseIndex
                        )
                        knowledgeDomains[domain].addClause(clause, standard)

            if not docSeries in DocTree.chapterIndex:
                DocTree.chapterIndex[docSeries] = {}
            if not partNr in DocTree.chapterIndex[docSeries]:
                DocTree.chapterIndex[docSeries][partNr] = {}
            DocTree.chapterIndex[docSeries][partNr][chapter] = clauseID

            depth = chapter.count(".")
            entry["clauseID"] = clauseID
            entry["depth"] = depth
            entry["title"] = title
            entry["typeID"] = typeID
            if depth == 0:
                context = "is top level content"
            elif depth == 1:
                parent_idx = chapter.rpartition(".")[0]
                context = f"is a subclause of clause \"{content[standard][partNr][parent_idx]['title']}\""
            else:
                parent_idx = chapter.rpartition(".")[0]
                top_idx = chapter.partition(".")[0]
                context = f"is contained in subclause \"{content[standard][partNr][parent_idx]['title']}\" under \"{content[standard][partNr][top_idx]['title']}\""
                # context = f"is part of \"{content[standard][partNr][parent_idx]['title']}\", which {content[standard][partNr][parent_idx]['context']}"
            entry["context"] = context
            if not standard in content:
                content[standard] = {}
                status[standard] = {}
            if not partNr in content[standard]:
                content[standard][partNr] = {}
                status[standard][partNr] = {}
            content[standard][partNr][chapter] = entry
            status[standard][partNr][chapter] = {}

            # print("{} \"{}\" is a {} of \"{}{}\" and {}.".format(clauseID, title, typedict[typeID], standard, part, context))


def load_doorstop_mapping(refmapfile=args.refmap):
    with open(refmapfile, newline="") as csvfile:
        refmapreader = csv.reader(csvfile, delimiter=";", quotechar="|")
        for row in refmapreader:
            doorstopID = row[1]
            clauseID = row[2]
            if clauseID not in Clause.clauseIndex:
                continue
            clause = Clause.clauseIndex[clauseID]
            clause.structure.doorstopID = doorstopID


def load_relations(relationsfile=args.relations):
    """
    Reads the relationsfile CSV list of precalculated relationships.

    We store all relations in one big class level dictionary Clause.relations.
    Alongside, we also calculate the "resonance" level of each clause which
    is basically the normalized cumulative score of all relations pointing to
    a particual clause.
    """
    if Clause.relations == None:
        Clause.relations = {}
    impulse = {}
    with open(relationsfile, newline="") as csvfile:
        relationsreader = csv.reader(csvfile, delimiter=";", quotechar="|")
        for row in relationsreader:
            fromClause = row[0]
            toClause = row[1]
            score = row[2]
            relation = (fromClause, toClause)
            if fromClause not in Clause.clauseIndex:
                continue
            source = Clause.clauseIndex[fromClause]
            peer = Clause.clauseIndex[toClause]
            homeDomain = knowledgeDomains[source.domain]
            homeDomain.memorizePeer(source,peer,score)
            Clause.relations[relation] = score
            if toClause in impulse:
                impulse[toClause] += float(score)
            else:
                impulse[toClause]=0
                impulse[toClause] += float(score)
    amplitude=0
    for toClause in impulse:
        amplitude += impulse[toClause]
    avg = amplitude / len(impulse)
    for toClause in impulse:
        target = Clause.clauseIndex[toClause]
        target.resonance = impulse[toClause] / avg


def load_weights(weightfile=args.weights):
    """
    Reads the weightfile CSV list of precalculated weight values for each
    sentence in the clause.
    """
    with open(weightfile, newline="") as csvfile:
        weightreader = csv.reader(csvfile, delimiter=";", quotechar="|")
        for row in weightreader:
            clauseID = row[0]
            scatter = row[1][1:-1]
            scatter = scatter.replace(" ", "")
            significance = row[2][1:-1]
            significance = significance.replace(" ", "")
            if clauseID in Clause.clauseIndex:
                clause = Clause.clauseIndex[clauseID]
            else:
                print(f"load_weights error {clauseID} missing")
                return
            scat = np.array(scatter.split(","), dtype=float)
            sign = np.array(significance.split(","), dtype=float)
            clause.scat = scat.tolist()
            clause.sign = sign.tolist()
            sentences = clause.getTokens()
            clause.sentences = sentences
            slen = len(sentences)
            if len(clause.scat) != slen:
                print(
                    f"{clause.structure.ID} scatter len mismatch {slen} {len(clause.scat)}"
                )
            if len(clause.sign) != slen:
                print(
                    f"{clause.structure.ID} significance len mismatch {slen} {len(clause.sign)}"
                )


docstore = {}


def parse_md_content():
    for standard in content.keys():
        std = standard.replace(" ", "")
        sec_head_pattern = r"^#+\s+([1-9A-Z][0-9.]*)\s?$|^#+\s+([1-9A-Z][0-9.]*)\s(.*)"
        sec_head_regex = re.compile(sec_head_pattern)
        for partNr in content[standard].keys():
            if partNr == "x":
                ptex = ""
            else:
                ptex = f"-{partNr}"

            inputfile = "markdown/" + std + ptex + ".md"
            outputfile = "markdown/" + std + ptex + "-gen.md"
            try:
                with open(inputfile, "r") as indata:
                    docstore[std] = {}
                    status[standard][partNr]["status"] = "parsed"
                    clauseID = ""
                    linebuffer = []
                    for line in indata:
                        line = line.rstrip()
                        chapter = ""
                        title = ""
                        if re.match("^#", line):
                            line = line.replace("Annex ", "")
                            match = sec_head_regex.match(line)
                            if match:
                                if match[1]:
                                    chapter = match[1].rstrip(".")
                                elif match[2]:
                                    chapter = match[2].rstrip(".")
                                    title = match[3]
                            else:
                                logger.info(f"header without match:\n\t{line}")
                                continue
                            if not chapter in DocTree.chapterIndex[std][partNr]:
                                logger.info(
                                    f"chapter without match: {std} {partNr} {chapter}\n\t{line}"
                                )
                                continue
                            clauseID = DocTree.chapterIndex[std][partNr][chapter]
                            clause = Clause.clauseIndex[clauseID]
                            if title != "":
                                clause.heading.addAlternative(
                                    title, "parsed", std + ptex + ".md"
                                )

                            if chapter in content[standard][partNr].keys():
                                # clauseID = content[standard][partNr][chapter]['clauseID']
                                clauseTitle = content[standard][partNr][chapter][
                                    "title"
                                ]
                                clauseType = content[standard][partNr][chapter][
                                    "typeID"
                                ]
                                clauseDepth = content[standard][partNr][chapter][
                                    "depth"
                                ]
                                clauseContext = content[standard][partNr][chapter][
                                    "context"
                                ]
                                status[standard][partNr][chapter]["status"] = "matched"
                            else:
                                logger.info(
                                    f"chapter without match: {clauseID}\n\t{line}"
                                )
                                continue
                            docstore[std][clauseID] = {}
                            docstore[std][clauseID]["text"] = []
                            content[standard][partNr][chapter]["text"] = []
                            docstore[std][clauseID]["context"] = clauseContext
                            docstore[std][clauseID]["type"] = clauseType
                            docstore[std][clauseID]["chapter"] = chapter
                            if title != "":
                                docstore[std][clauseID]["title"] = title
                                if title != clauseTitle and clauseTitle not in (
                                    "TOC",
                                    "REQUIREMENT",
                                    "OBJECTIVE",
                                    "SCOPE",
                                    "TERM",
                                ):
                                    logger.warning(
                                        f"clause title mismatch: {clauseID}\n\t{title}\n\t{clauseTitle}"
                                    )
                            elif clauseTitle not in (
                                "TOC",
                                "REQUIREMENT",
                                "OBJECTIVE",
                                "SCOPE",
                                "TERM",
                            ):
                                logger.info(
                                    f"overriding missing title for {clauseID} with clauseTitle {clauseTitle}"
                                )
                                docstore[std][clauseID]["title"] = clauseTitle
                            else:
                                logger.warning(f"no title found for {clauseID}")
                                docstore[std][clauseID]["title"] = "none"
                        elif clauseID != "":
                            clause = Clause.clauseIndex[clauseID]
                            clause.addText(line)
                            docstore[std][clauseID]["text"].append(line)
                            chapter = docstore[std][clauseID]["chapter"]
                            content[standard][partNr][chapter]["text"].append(line)

            except IOError as e:
                logger.warning(f"file open error: {e}")


def missed_headers():
    for standard in content.keys():
        for partNr in content[standard].keys():
            if not "status" in status[standard][partNr]:
                continue
            for chapter in content[standard][partNr].keys():
                if "status" in status[standard][partNr][chapter]:
                    idx_status = status[standard][partNr][chapter]["status"]
                    continue
                # logger.warning(f'missing MD heading {content[standard][partNr][chapter]['clauseID']}')
                print(f"missing MD heading for {standard} {partNr} clause {chapter}")


bulk_headings = "bulk_headings"
new_headings = "new_headings"


def check_clause_type():
    for clauseID in Clause.clauseIndex.keys():
        clause = Clause.clauseIndex[clauseID]
        text = clause.getText()
        clauseType = clause.type
        if re.match(".*shall.*", text, re.IGNORECASE):
            if clauseType != "r":
                print(f"{clauseID} type {clauseType} may be a requirement")
        if re.match(".*objective.*", text, re.IGNORECASE):
            if clauseType != "o":
                print(f"{clauseID} type {clauseType} may be a objective")
        if re.match(".*scope.*", text, re.IGNORECASE):
            if clauseType != "s":
                print(f"{clauseID} type {clauseType} may be a scope definition")


def generate_headings():
    for std in docstore.keys():
        llama_hl_pattern = r'^[^"]*"(.+)"\W?$'
        offer_regex = re.compile(llama_hl_pattern)
        for clauseID in docstore[std].keys():
            title = docstore[std][clauseID]["title"]
            offer = []
            if title == "none":
                print(".", end="", flush=True)
                clauseType = typedict[docstore[std][clauseID]["type"]]
                text = " ".join(docstore[std][clauseID]["text"])

                while len(offer) == 0:
                    # response = ollama.generate(model='nemotron',
                    # response = ollama.generate(model='llama3.1',
                    #response = ollama.generate(
                    #    model="granite3-moe",
                        # response = ollama.generate(model='granite3-dense',
                        # response = ollama.generate(model='hf.co/ibm-granite/granite-8b-code-instruct-4k-GGUF',
                    #    prompt=f"create a max 3 word headline for the following {clauseType}: {text}",
                    #)
                    prompt=f"create a max 3 word headline for the following {clauseType}: {text}"

                    response = llm.query(prompt)
                    # print(response['response'])
                    for line in response["response"].splitlines():
                        match = offer_regex.match(line)
                        if match:
                            offer.append(match[1])
                    if len(offer) == 0:
                        print(
                            f"no offer for {clauseID} in response\n>{response['response']}<"
                        )
                title = offer[0]
                docstore[std][clauseID]["title"] = title


def generate_alternative_headings():
    with open(new_headings, "w") as store:
        nemotron_hl_pattern = r"^[1-9]\.\s+\*+([\w\s.&-/]+)\*+"
        offer_regex = re.compile(nemotron_hl_pattern)
        for clauseID in Clause.clauseIndex.keys():
            clause = Clause.clauseIndex[clauseID]
            if clause.heading.isSpecific():
                continue
            text = clause.text
            if len(text) == 0:
                continue
            title = clause.heading.display
            print(".", end="", flush=True)
            offer = []
            store.write(f"# {clauseID}\n")
            while len(offer) == 0:
                response = ollama.generate(
                    model="nemotron",
                    prompt=f"create a max 3 word heading for the following {clause.type}: {text}",
                )
                store.write(response["response"])
                store.write("\n\n")
                for line in response["response"].splitlines():
                    match = offer_regex.match(line)
                    if match:
                        offer.append(match[1])
            for i in range(len(offer)):
                clause.heading.addAlternative(offer[i], "generated", "nemotron")


def generate_interactive_headings():
    with open(new_headings, "w") as store:
        for std in docstore.keys():
            nemotron_hl_pattern = r"^[1-9]\.\s+\*+([\w\s]+)\*+"
            llama_hl_pattern = r'^"([\w\s]+)"'
            # offer_regex = re.compile(nemotron_hl_pattern)
            offer_regex = re.compile(llama_hl_pattern)
            for clauseID in docstore[std].keys():
                title = docstore[std][clauseID]["title"]
                if title == "none":
                    clauseType = typedict[docstore[std][clauseID]["type"]]
                    text = " ".join(docstore[std][clauseID]["text"])
                    print(f"missing heading for {clauseID}\n")
                    print(text)
                    print("\n")
                    answer = 0
                    offer = []
                    while int(answer) < 1 or int(answer) > len(offer):
                        # response = ollama.generate(model='nemotron',
                        # response = ollama.generate(model='llama3.1',
                        #response = ollama.generate(
                          #  model="granite3-moe",
                            # response = ollama.generate(model='granite3-dense',
                            # response = ollama.generate(model='hf.co/ibm-granite/granite-8b-code-instruct-4k-GGUF',
                         #   prompt=f"create a max 3 word headline for the following {clauseType}: {text}",
                        #)
                        prompt=f"create a max 3 word headline for the following {clauseType}: {text}"
                        response = llm.query(prompt)
                        print(response["response"])
                        offer = []
                        for line in response["response"].splitlines():
                            match = offer_regex.match(line)
                            if match:
                                offer.append(match[1])
                        for i in range(len(offer)):
                            print(i + 1, "   ", offer[i])
                        answer = input("make your choice: ")
                        if answer == "exit":
                            store.close()
                            sys.exit()
                        elif answer == "":
                            answer = 0
                    title = offer[int(answer) - 1]
                md5hash = hashlib.md5(clauseID.encode("utf-8")).hexdigest()
                entry = f"TOC;{md5hash};{clauseID};{title};{docstore[std][clauseID]['type']}\n"
                print(entry)
                store.write(entry)


def generate_bulk_headings():
    with open(new_headings, "w") as store:
        for std in docstore.keys():
            nemotron_hl_pattern = r"^[1-9]\.\s+\*+([\w\s]+)\*+"
            llama_hl_pattern = r'^"([\w\s]+)"'
            offer_regex = re.compile(nemotron_hl_pattern)
            # offer_regex = re.compile(llama_hl_pattern)
            for clauseID in docstore[std].keys():
                title = docstore[std][clauseID]["title"]
                if title == "none":
                    print(".", end="", flush=True)
                    clauseType = typedict[docstore[std][clauseID]["type"]]
                    text = " ".join(docstore[std][clauseID]["text"])
                    store.write(f"# {clauseID}\n")
                    #response = ollama.generate(
                    #    model="nemotron",
                        # response = ollama.generate(model='llama3.1',
                    #    prompt=f"create a max 3 word heading for the following {clauseType}: {text}",
                    #)
                    prompt=f"create a max 3 word heading for the following {clauseType}: {text}"
                    response = llm.query(prompt)
                    store.write(response["response"])
                    store.write("\n\n")


def select_from_offer(std, clauseID, offer=[]):
    nemotron_hl_pattern = r"^[1-9]\.\s+\*+([\w\s]+)\*+"
    offer_regex = re.compile(nemotron_hl_pattern)
    for i in range(len(offer)):
        print(i + 1, "   ", offer[i])
    answer = input("make your choice: ")
    if answer == "exit":
        sys.exit()
    elif len(answer) > 7:
        print(f"returning custom answer {answer}")
        return answer
    elif answer == "":
        answer = 0
    while int(answer) < 1 or int(answer) > len(offer):
        clauseType = typedict[docstore[std][clauseID]["type"]]
        text = " ".join(docstore[std][clauseID]["text"])
        response = ollama.generate(
            model="nemotron",
            prompt=f"create a max 3 word headline for the following {clauseType}: {text}",
        )
        
        offer = []
        for line in response["response"].splitlines():
            match = offer_regex.match(line)
            if match:
                offer.append(match[1])
            for i in range(len(offer)):
                print(i + 1, "   ", offer[i])
            answer = input("make your choice: ")
            if answer == "exit":
                sys.exit()
            elif len(answer) > 7:
                print(f"returning custom answer {answer}")
                return answer
            elif answer == "":
                answer = 0
            else:
                print(f"standard answer {answer}")
    return offer[int(answer) - 1]


def alternatives_from_bulk_headings():
    with open(bulk_headings, "r") as bulk:
        nemotron_hl_pattern = r"^[1-9]\.\s+\*+([\w\s.&]+)\*+"
        offer_regex = re.compile(nemotron_hl_pattern)
        std = ""
        clauseID = ""
        text = ""
        offer = []
        for line in bulk:
            if re.match("^#", line):
                clauseID = line[2:].rstrip()
            else:
                match = offer_regex.match(line)
                if match:
                    clause = clauseIndex[clauseID]
                    display = clause.heading.display
                    clause.heading.addAlternative(match[1], "generated", "nemotron")


def process_bulk_headings():
    with open(bulk_headings, "r") as bulk:
        with open(new_headings, "w") as select:
            nemotron_hl_pattern = r"^[1-9]\.\s+\*+([\w\s]+)\*+"
            llama_hl_pattern = r'^"([\w\s]+)"'
            offer_regex = re.compile(nemotron_hl_pattern)
            # offer_regex = re.compile(llama_hl_pattern)
            std = ""
            clauseID = ""
            text = ""
            offer = []
            for line in bulk:
                if re.match("^#", line):
                    if len(offer) > 0:
                        print(f"\nSelect heading for {clauseID} text\n")
                        print(text, "\n")
                        heading = select_from_offer(std, clauseID, offer)
                        docstore[std][clauseID]["title"] = heading
                        md5hash = hashlib.md5(clauseID.encode("utf-8")).hexdigest()
                        entry = f"TOC;{md5hash};{clauseID};{heading};{docstore[std][clauseID]['type']}\n"
                        print(entry)
                        select.write(entry)
                    clauseID = line[2:].rstrip()
                    match = iso_regex.match(clauseID)
                    if match:
                        standard = match[1]
                        std = standard.replace(" ", "")
                        if match[2] == "":
                            part = ""
                            partNr = "x"
                        else:
                            part = " part " + match[2]
                            partNr = match[2]
                        chapter = match[3]
                        text = " ".join(docstore[std][clauseID]["text"])
                        answer = 0
                        offer = []
                    else:
                        print(f"bulk parse error for line {line}")
                        continue
                else:
                    match = offer_regex.match(line)
                    if match:
                        offer.append(match[1])


def write_md_documents():
    for standard in content.keys():
        std = standard.replace(" ", "")
        if not std in docstore.keys():
            continue
        for partNr in content[standard].keys():
            if not "status" in status[standard][partNr]:
                continue
            if partNr == "x":
                ptex = ""
            else:
                ptex = f"-{partNr}"
            md_document = "markdown/" + std + ptex + "-gen.md"
            with open(md_document, "w") as output:
                print(f"writing {md_document}")
                for chapter in natsorted(content[standard][partNr]):
                    clauseID = content[standard][partNr][chapter]["clauseID"]
                    print(f"working on {standard}~{partNr}~{std}~{chapter}")
                    if clauseID in docstore[std].keys():
                        title = docstore[std][clauseID]["title"]
                    else:
                        title = content[standard][partNr][chapter]["title"]
                    text = ""
                    if "text" in content[standard][partNr][chapter].keys():
                        text = "\n".join(content[standard][partNr][chapter]["text"])
                    depth = int(content[standard][partNr][chapter]["depth"]) + 1
                    context = content[standard][partNr][chapter]["context"]
                    heading = "#" * depth
                    heading += " "
                    heading += chapter
                    heading += " "
                    heading += title
                    # print(f"writing {heading}")
                    output.write(heading)
                    output.write(text)
                    output.write("\n")
            output.close()


def dumpClauseIndex():
    return json.dumps(
        Clause.clauseIndex, default=lambda o: o.__dict__, sort_keys=True, indent=4
    )


if __name__ == "__main__":
    # options depend on the locally loaded models for ollama. Examples include:
    #   'llama3.1'          fast, generates up to three alternatives
    #   'nemotron'          slow, generates three to five alternatives, best quality
    #   'granite3-moe'      fast, generates one heading, ignores the word limit
    #   'granite3-dense'    fast, generates one and occasionally more headings, ignores the word limit
    #   'hf.co/ibm-granite/granite-8b-code-instruct-4k-GGUF'    multiline free dialog response
    heading_factory = HeadingFactory("nemotron")
    load_content_structure()
    parse_md_content()
    load_weights()
    load_relations()
    load_doorstop_mapping()
    heading_factory.load_alternatives_from_file("bulk_headings")
    # heading_factory.headings4all(cacheFile='new_headings', verbose=True)
    missed_headers()
    # check_clause_type()
    # generate_headings()
    # generate_interactive_headings()
    # generate_alternative_headings()
    # generate_bulk_headings()
    # process_bulk_headings()
    # write_md_documents()
    for domain in knowledgeDomains:
        knowledgeDomains[domain].docTree.deleteEmptySeries()
        # print(knowledgeDomains[domain])
        # knowledgeDomains[domain].dumpKnowledgeHeadings()
        # knowledgeDomains[domain].dumpKnowledgeNodes()
        # knowledgeDomains[domain].dumpKnowledgeEdges()
        # knowledgeDomains[domain].ingestDomainClauses()
        # knowledgeDomains[domain].summarizeClauses(verbose=True)
        # knowledgeDomains[domain].relateClauses()
        knowledgeDomains[domain].findClusters()
        rel.clusterDump(Clause.clauseIndex)
        # knowledgeDomains[domain].dumpSumstore(cacheFile="sumstore-"+domain+".json")
        # knowledgeDomains[domain].dumpKnowledgeTexts()
        # knowledgeDomains[domain].printKnowledgeTexts()
        # print(knowledgeDomains[domain].docTree.docSeriesWeight())
        # print(knowledgeDomains[domain].docTree.docSeriesSize())
    # nodes = knowledgeDomains['automotive'].retrieve("")
    # EN 50716 7.6.4.7
    # nodes = knowledgeDomains['industry'].retrieve("To the extent required by the software integrity level (see Table A.1), a Software/Hardware Integration Test Report shall be written, under the responsibility of the Tester, on the basis of the Software/Hardware Integration Test Specification. Requirements from 7.6.4.8 to 7.6.4.10 refer to the Software/Hardware Integration Test Report.")
    # nodes = knowledgeDomains['railway'].retrieve("The objective of the qualification of software components is to provide evidence for their suitability for re-use in items developed in compliance with the ISO 26262 series of standards. The use of qualified software components avoids re-development of existing software components with identical functionalities or properties or enables the use of general purpose commercial off-the-shelf (COTS) software. Software components addressed by this clause include: - software libraries from third-party suppliers [commercial off-the-shelf (COTS) software]; - already existing SW components not developed according to ISO 26262; - in-house components already in use in electronic control units.")
    # for node in nodes:
    #     print(node)
    # print(dumpClauseIndex())
